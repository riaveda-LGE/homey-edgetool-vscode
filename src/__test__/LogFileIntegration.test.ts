// === src/__test__/LogFileIntegration.test.ts ===
import type { LogEntry } from '@ipc/messages';
import * as fs from 'fs';
import * as path from 'path';

import { countTotalLinesInDir, mergeDirectory } from '../core/logs/LogFileIntegration.js';
import type { ParserConfig } from '../core/logs/ParserEngine.js';
import {
  cleanAndEnsureDir,
  cleanDir,
  drainNextTicks,
  prepareUniqueOutDir,
} from './helpers/testFs.js';
import { PARSER_TEMPLATE_REL } from '../shared/const.js';

jest.setTimeout(60_000);

// â”€â”€ ë©”ì‹œì§€ ë¹„êµ ì •ê·œí™”(ANSI ì œê±° + ê³µë°± ì¶•ì•½ + trim) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const ANSI_RE = /\u001b\[[0-9;]*m/g; // \x1b[...m
function normalizeText(s: string): string {
  return s.replace(ANSI_RE, '').replace(/[ \t]+/g, ' ').trim();
}

// ê³¨ë“  ìë™ ê°±ì‹ (ì‹¤ì œ ê²°ê³¼ë¡œ after_merge/merged.log ë®ì–´ì“°ê¸°)
const UPDATE_GOLDEN = process.env.UPDATE_GOLDEN === '1';

// â”€â”€ merged.log í•œ ì¤„ì„ (time/process/pid/message)ë¡œ ë¶„í•´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// í˜•ì‹: "[Mon DD HH:MM:SS(.mmm)] process[pid]?: message"
const MERGED_LINE_RE =
  /^\[(?<time>[A-Za-z]{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}(?:\.\d{3,6})?)\]\s+(?<proc>[A-Za-z0-9._-]+)(?:\[(?<pid>\d+)\])?:\s+(?<msg>.*)$/;
type ExpectedParsed = { time: string | null; process: string | null; pid: string | null; message: string | null };
function parseMergedLine(line: string): ExpectedParsed {
  const m = MERGED_LINE_RE.exec(line);
  if (!m?.groups) return { time: null, process: null, pid: null, message: line };
  return {
    time: m.groups.time ?? null,               // ì´ë¯¸ ëŒ€ê´„í˜¸ ë‚´ë¶€ë§Œ ìº¡ì³ë¨
    process: m.groups.proc ?? null,
    pid: m.groups.pid ?? null,
    message: m.groups.msg ?? null,
  };
}
function extractMergedHeader(line: string): string | null {
  const m = MERGED_LINE_RE.exec(line);
  if (!m?.groups) return null;
  const pid = m.groups.pid ? `[${m.groups.pid}]` : '';
  return `[${m.groups.time}] ${m.groups.proc}${pid}: `;
}

// â¬‡ï¸ ì£¼ì˜: ì´ í†µí•© í…ŒìŠ¤íŠ¸ëŠ” **ì›ë³¸ ë¼ì¸ ì „ì²´**(time/process/pid/message)ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
// parserëŠ” ë‚´ì¥ í…œí”Œë¦¿(JSON)ì„ ë¡œë“œí•´ì„œ ì‚¬ìš©í•˜ê³ , mergeDirectory ì˜µì…˜ `preserveFullText`ë¡œ
// í—¤ë”ë¥¼ ë³µì›í•´ ë¹„êµí•©ë‹ˆë‹¤(ìš´ì˜ê³¼ ë™ì¼í•œ ParserEngine ê²½ë¡œë¥¼ ê°•ì œ).

let OUT_DIR: string;

// í…ŒìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©í•  íŒŒì„œ ì„¤ì •ì„ í…œí”Œë¦¿ JSONìœ¼ë¡œë¶€í„° ë¡œë“œ
function loadTemplateParserConfig(): ParserConfig {
  // __dirname: src/__test__ â†’ ë‘ ë‹¨ê³„ ìƒìœ„ê°€ repo root
  const templatePath = path.resolve(__dirname, '..', '..', PARSER_TEMPLATE_REL);
  const buf = fs.readFileSync(templatePath, 'utf8');
  const json = JSON.parse(buf);
  return json as ParserConfig;
}

async function runMergeTest(testName: string, testSuiteDir: string, outputFileName: string) {
  const testDir = path.resolve(__dirname, 'test_log', testSuiteDir);
  const inputDir = path.join(testDir, 'before_merge');
  const expectedOutputPath = path.join(testDir, 'after_merge', 'merged.log');

  const expectedContent = fs.readFileSync(expectedOutputPath, 'utf8');
  const expectedLines = expectedContent
    .trim()
    .split('\n')
    .map((line) => line.replace(/\r$/, ''))
    .filter((line) => line.length > 0);
  // ê¸°ëŒ€ ìŠ¤ëƒ…ìƒ·(plain text) â†’ êµ¬ì¡°í™”
  const expectedParsed = expectedLines.map(parseMergedLine).map((p) => ({
    ...p,
    message: p.message != null ? normalizeText(p.message) : null,
  }));

  console.log(
    `ğŸ“Š ${testName} - Expected lines: ${expectedLines.length}, first line: "${expectedLines[0]}"`,
  );

  const outDir = OUT_DIR;
  const mergedDir = path.join(OUT_DIR, 'merged');
  fs.mkdirSync(mergedDir, { recursive: true });

  // ë””ë²„ê·¸ ì‚°ì¶œë¬¼ í´ë”
  const debugDir = path.join(OUT_DIR, 'debug');
  fs.mkdirSync(debugDir, { recursive: true });
  console.log(`ğŸ§ª debug artifacts â†’ ${debugDir}`);

  const actualResults: LogEntry[] = [];
  const outputPath = path.join(OUT_DIR, outputFileName);
  const outputStream = fs.createWriteStream(outputPath, { encoding: 'utf8' });

  // ì§„í–‰ ê°ì‹œ(ë¬´ì§„í–‰ watchdog)
  let lastProgressAt = Date.now();

  const onBatch = (logs: LogEntry[]) => {
    actualResults.push(...logs);
    for (const log of logs) outputStream.write(log.text + '\n');
    lastProgressAt = Date.now();
  };

  // â±ï¸ ë¬´ì§„í–‰ ê°ì‹œ: 45ì´ˆ + merged/*.jsonl í¬ê¸° ì¦ê°€ë¥¼ ì§„í–‰ìœ¼ë¡œ ì¸ì •(ì˜¤íƒ ë°©ì§€)
  const WATCHDOG_MS = 45_000;
  let watchdogTimer: NodeJS.Timeout | undefined;
  let statTimer: NodeJS.Timeout | undefined;
  // mergedDir ì•ˆì˜ .jsonl íŒŒì¼ í¬ê¸°ê°€ ì¦ê°€í•˜ë©´ ì§„í–‰ìœ¼ë¡œ ê°„ì£¼
  const mergedSizes = new Map<string, number>();
  statTimer = setInterval(() => {
    try {
      const names = fs.existsSync(mergedDir) ? fs.readdirSync(mergedDir) : [];
      for (const n of names) {
        if (!n.endsWith('.jsonl')) continue;
        const fp = path.join(mergedDir, n);
        const st = fs.statSync(fp);
        const prev = mergedSizes.get(n) || 0;
        if (st.size > prev) {
          mergedSizes.set(n, st.size);
          lastProgressAt = Date.now();
        }
      }
    } catch {}
  }, 500);

  const watchdog = new Promise<void>((_resolve, reject) => {
    watchdogTimer = setInterval(() => {
      if (Date.now() - lastProgressAt > WATCHDOG_MS) {
        clearInterval(watchdogTimer!);
        if (statTimer) clearInterval(statTimer);
        reject(
          new Error(
            `Watchdog: no progress for ${WATCHDOG_MS / 1000}s (test="${testName}", suite="${testSuiteDir}")`,
          ),
        );
      }
    }, 1000);
  });

  await Promise.race([
    mergeDirectory({
      dir: inputDir,
      mergedDirPath: mergedDir,
      onBatch,
      batchSize: 1000,
      // âœ… ë‚´ì¥ í…œí”Œë¦¿ íŒŒì„œ ì ìš© + í—¤ë” ë³µì›: ìš´ì˜ ê²½ë¡œì™€ ë™ì¼í•˜ê²Œ ParserEngineì„ ê±°ì¹˜ë˜
      //    í…ŒìŠ¤íŠ¸ ë¹„êµëŠ” ì „ì²´ í—¤ë” í˜•íƒœë¡œ ìˆ˜í–‰
      parser: loadTemplateParserConfig(),
      preserveFullText: true,
    }),
    watchdog,
  ]).finally(() => {
    if (watchdogTimer) clearInterval(watchdogTimer);
    if (statTimer) clearInterval(statTimer);
  });

  await drainNextTicks();
  await new Promise<void>((resolve) => { outputStream.end(() => resolve()); });

  expect(actualResults.length).toBe(expectedLines.length);

  // mergeDirectoryëŠ” "ìµœì‹ â†’ì˜¤ë˜ëœ(desc)"ìœ¼ë¡œ ë°°ì¶œí•œë‹¤.
  // ê¸°ëŒ€ ìŠ¤ëƒ…ìƒ·ë„ "ìµœì‹ â†’ì˜¤ë˜ëœ(desc)"ì´ë¯€ë¡œ ê°™ì€ ì¸ë±ìŠ¤ë¡œ ë¹„êµí•œë‹¤.
  const actualTexts = actualResults.map((e) => e.text);
  const actualParsed = actualTexts.map(parseMergedLine).map((p) => ({
    ...p,
    message: p.message != null ? normalizeText(p.message) : null,
  }));

  // ğŸ” ë””ë²„ê¹… ì‚°ì¶œë¬¼
  // 1) ì›ë³¸(raw)
  fs.writeFileSync(
    path.join(debugDir, `${outputFileName}.expected_raw.log`),
    expectedLines.join('\n') + '\n',
    'utf8',
  );
  fs.writeFileSync(
    path.join(debugDir, `${outputFileName}.actual_raw.log`),
    actualTexts.join('\n') + '\n',
    'utf8',
  );
  // 2) message-only
  const expectedMsgs = expectedLines.map((ln) => parseMergedLine(ln).message ?? ln);
  const actualMsgs = actualTexts.map((ln) => parseMergedLine(ln).message ?? ln);
  fs.writeFileSync(
    path.join(debugDir, `${outputFileName}.expected_message_only.log`),
    expectedMsgs.join('\n') + '\n',
    'utf8',
  );
  fs.writeFileSync(
    path.join(debugDir, `${outputFileName}.actual_message_only.log`),
    actualMsgs.join('\n') + '\n',
    'utf8',
  );
  // 3) êµ¬ì¡°í™” JSON
  fs.writeFileSync(
    path.join(debugDir, `${outputFileName}.expected_structured.json`),
    JSON.stringify(expectedParsed, null, 2),
    'utf8',
  );
  fs.writeFileSync(
    path.join(debugDir, `${outputFileName}.actual_structured.json`),
    JSON.stringify(actualParsed, null, 2),
    'utf8',
  );
  // ê¸°ì¡´ í—¤ë”ë¥¼ ìœ ì§€í•˜ê³ , bodyë§Œ ì‹¤ì œ ê²°ê³¼ë¡œ ëŒ€ì²´í•œ "ìŠ¤ëƒ…ìƒ· ì œì•ˆë³¸"
  const snapshotSuggested = expectedLines.map((ln, i) => {
    const header = extractMergedHeader(ln);
    const body = actualTexts[i] ?? '';
    return header ? header + body : body;
  });
  fs.writeFileSync(
    path.join(debugDir, `snapshot_suggested_new_${outputFileName}`),
    snapshotSuggested.join('\n') + '\n',
    'utf8',
  );

  const mismatches: string[] = [];
  for (let i = 0; i < expectedLines.length; i++) {
    const exp = expectedParsed[i]!;
    const act = actualParsed[i]!;
    const fieldsMatch =
      exp.time === act.time &&
      exp.process === act.process &&
      exp.pid === act.pid &&
      exp.message === act.message;

    if (!fieldsMatch) {
      mismatches.push(
        `Entry ${i + 1} (parsed field mismatch):\n` +
          `  Expected: time="${exp.time}", process="${exp.process}", pid="${exp.pid}", message="${exp.message}"\n` +
          `  Actual:   time="${act.time}", process="${act.process}", pid="${act.pid}", message="${act.message}"\n` +
          `  Raw expected: "${expectedLines[i]}"\n` +
          `  Raw actual:   "${actualTexts[i]}"`,
      );
      if (mismatches.length >= 20) break;
    }
  }
  if (mismatches.length > 0) {
    if (UPDATE_GOLDEN) {
      // ì‹¤ì œ ê²°ê³¼ë¡œ golden ê°±ì‹ 
      fs.writeFileSync(expectedOutputPath, actualTexts.join('\n') + '\n', 'utf8');
      // êµ¬ì¡°í™” ê¸°ëŒ€ê°’ë„ í•¨ê»˜ ë‚¨ê¹€(ì°¸ê³ ìš©)
      const structuredOut = path.join(path.dirname(expectedOutputPath), 'merged_structured.golden.json');
      fs.writeFileSync(structuredOut, JSON.stringify(actualParsed, null, 2), 'utf8');
      console.log(`ğŸŸ¡ Golden updated: ${expectedOutputPath}`);
      console.log(`ğŸŸ¡ Structured golden: ${structuredOut}`);
      return;
    }
    throw new Error(`Test failed with ${mismatches.length} line mismatches\n` + mismatches.join('\n'));
  }

  console.log(`âœ… ${testName} passed: ${actualResults.length} lines merged correctly`);
}

describe('LogFileIntegration', () => {
  beforeEach(() => {
    OUT_DIR = prepareUniqueOutDir('lfi');
    cleanAndEnsureDir(OUT_DIR);
  });
  afterEach(() => {
    cleanDir(OUT_DIR);
  });

  describe('mergeDirectory í•¨ìˆ˜', () => {
    it('ì¼ë°˜ ë¡œê·¸ íŒŒì¼ë“¤ì„ ì •í™•íˆ ë³‘í•©í•´ì•¼ í•¨', async () => {
      await runMergeTest('Normal test', 'normal_test_suite', 'normal_result_merged.log');
    }, 60_000);

    it('íƒ€ì„ì¡´ ì í”„ê°€ ìˆëŠ” ë¡œê·¸ íŒŒì¼ë“¤ì„ ì •í™•íˆ ë³‘í•©í•´ì•¼ í•¨', async () => {
      await runMergeTest('Timezone test', 'timezone_jump_test_suite', 'timezone_result_merged.log');
    }, 60_000);

    it('ë¹ˆ ë””ë ‰í„°ë¦¬ë¥¼ gracefully ì²˜ë¦¬í•´ì•¼ í•¨', async () => {
      const tempDir = path.join(OUT_DIR, 'temp_empty');
      cleanAndEnsureDir(tempDir);

      const mergedDir = path.join(OUT_DIR, 'merged');
      cleanAndEnsureDir(mergedDir);

      const onBatch = jest.fn((logs: LogEntry[]) => {
        expect(Array.isArray(logs)).toBe(true);
        expect(logs.length).toBe(0);
      });

      await mergeDirectory({ dir: tempDir, onBatch, mergedDirPath: mergedDir });

      expect(onBatch).not.toHaveBeenCalled();
    }, 60_000);

    it('ì¤‘ë‹¨ ì‹ í˜¸ë¥¼ ì œëŒ€ë¡œ ì²˜ë¦¬í•´ì•¼ í•¨', async () => {
      const testDir = path.resolve(__dirname, 'test_log', 'normal_test_suite');
      const inputDir = path.join(testDir, 'before_merge');

      const mergedDir = path.join(OUT_DIR, 'merged');
      cleanAndEnsureDir(mergedDir);

      const abortController = new AbortController();
      let batchCount = 0;
      let abortedAt: number | null = null;
      let postAbortBatches = 0;
      let emittedLines = 0;

      const onBatch = (logs: LogEntry[]) => {
        if (abortedAt !== null) {
          postAbortBatches++;
          return;
        }
        batchCount++;
        emittedLines += logs.length;
        if (batchCount >= 3 && !abortController.signal.aborted) {
          abortedAt = batchCount;
          abortController.abort();
        }
      };

      await expect(
        mergeDirectory({
          dir: inputDir,
          onBatch,
          signal: abortController.signal,
          batchSize: 1,
          mergedDirPath: mergedDir,
        }),
      ).resolves.toBeUndefined();

      expect(abortedAt).not.toBeNull();
      expect(postAbortBatches).toBe(0);
      expect(batchCount).toBe(abortedAt);
      const { total } = await countTotalLinesInDir(inputDir);
      expect(emittedLines).toBeLessThan(total);
    }, 60_000);
  });
});